{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "fc8fede8-6ce8-43a9-8cd7-c9bcc798e0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = '''This article is about natural language processing done by computers. For the natural language processing done by the human brain, see Language processing in the brain.\n",
    "An automated online assistant providing customer service on a web page, an example of an application where natural language processing is a major component.\n",
    "Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data. The goal is a computer capable of \"understanding\" the contents of documents, including the contextual nuances of the language within them. The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves.\n",
    "Challenges in natural language processing frequently involve speech recognition, natural language understanding, and natural language generation.\n",
    "you would've had done that, isn\\'t it?\n",
    "'''.strip('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b6c2719c-8f3a-4c19-9e78-610b2d328b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This article is about natural language processing done by computers. For the natural language processing done by the human brain, see Language processing in the brain.\n",
      "An automated online assistant providing customer service on a web page, an example of an application where natural language processing is a major component.\n",
      "Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data. The goal is a computer capable of \"understanding\" the contents of documents, including the contextual nuances of the language within them. The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves.\n",
      "Challenges in natural language processing frequently involve speech recognition, natural language understanding, and natural language generation.\n",
      "you would've had done that, isn't it?\n"
     ]
    }
   ],
   "source": [
    "print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "9e9be669-29a1-4e25-99ff-9259ebdb5e3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\boss\\anaconda3\\lib\\site-packages (3.6.1)\n",
      "Requirement already satisfied: click in c:\\users\\boss\\anaconda3\\lib\\site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: regex in c:\\users\\boss\\anaconda3\\lib\\site-packages (from nltk) (2021.4.4)\n",
      "Requirement already satisfied: tqdm in c:\\users\\boss\\anaconda3\\lib\\site-packages (from nltk) (4.59.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\boss\\anaconda3\\lib\\site-packages (from nltk) (1.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "8427e114-92d6-440e-95f8-0fedaa216e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c85afb69-afc3-41ca-84ac-db0859221a4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "f2e24f1b-94bd-4055-a3fd-7f08cb3f2b92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This article is about natural language processing done by computers.',\n",
       " 'For the natural language processing done by the human brain, see Language processing in the brain.',\n",
       " 'An automated online assistant providing customer service on a web page, an example of an application where natural language processing is a major component.',\n",
       " 'Natural language processing (NLP) is a subfield of linguistics, computer science, and artificial intelligence concerned with the interactions between computers and human language, in particular how to program computers to process and analyze large amounts of natural language data.',\n",
       " 'The goal is a computer capable of \"understanding\" the contents of documents, including the contextual nuances of the language within them.',\n",
       " 'The technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves.',\n",
       " 'Challenges in natural language processing frequently involve speech recognition, natural language understanding, and natural language generation.',\n",
       " \"you would've had done that, isn't it?\"]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents = nltk.sent_tokenize(txt)\n",
    "sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "9d97cec5-53bf-44d5-808a-38703713bd5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "19dcefbc-bc0c-49da-b608-387b6895f290",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This article is about natural language processing done by computers.']"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = nltk.sent_tokenize(sents[0])\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "f0ec6e3d-8456-4a4d-8a78-251d4cfd0fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "words=[]\n",
    "for sent in nltk.sent_tokenize(txt):\n",
    "    ws = nltk.word_tokenize(txt)\n",
    "    words = (ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "83b43898-205c-4f42-b580-2dc468d41443",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'article',\n",
       " 'is',\n",
       " 'about',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'done',\n",
       " 'by',\n",
       " 'computers',\n",
       " '.',\n",
       " 'For',\n",
       " 'the',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'done',\n",
       " 'by',\n",
       " 'the',\n",
       " 'human',\n",
       " 'brain',\n",
       " ',',\n",
       " 'see',\n",
       " 'Language',\n",
       " 'processing',\n",
       " 'in',\n",
       " 'the',\n",
       " 'brain',\n",
       " '.',\n",
       " 'An',\n",
       " 'automated',\n",
       " 'online',\n",
       " 'assistant',\n",
       " 'providing',\n",
       " 'customer',\n",
       " 'service',\n",
       " 'on',\n",
       " 'a',\n",
       " 'web',\n",
       " 'page',\n",
       " ',',\n",
       " 'an',\n",
       " 'example',\n",
       " 'of',\n",
       " 'an',\n",
       " 'application',\n",
       " 'where',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'is',\n",
       " 'a',\n",
       " 'major',\n",
       " 'component',\n",
       " '.',\n",
       " 'Natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " '(',\n",
       " 'NLP',\n",
       " ')',\n",
       " 'is',\n",
       " 'a',\n",
       " 'subfield',\n",
       " 'of',\n",
       " 'linguistics',\n",
       " ',',\n",
       " 'computer',\n",
       " 'science',\n",
       " ',',\n",
       " 'and',\n",
       " 'artificial',\n",
       " 'intelligence',\n",
       " 'concerned',\n",
       " 'with',\n",
       " 'the',\n",
       " 'interactions',\n",
       " 'between',\n",
       " 'computers',\n",
       " 'and',\n",
       " 'human',\n",
       " 'language',\n",
       " ',',\n",
       " 'in',\n",
       " 'particular',\n",
       " 'how',\n",
       " 'to',\n",
       " 'program',\n",
       " 'computers',\n",
       " 'to',\n",
       " 'process',\n",
       " 'and',\n",
       " 'analyze',\n",
       " 'large',\n",
       " 'amounts',\n",
       " 'of',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'data',\n",
       " '.',\n",
       " 'The',\n",
       " 'goal',\n",
       " 'is',\n",
       " 'a',\n",
       " 'computer',\n",
       " 'capable',\n",
       " 'of',\n",
       " '``',\n",
       " 'understanding',\n",
       " \"''\",\n",
       " 'the',\n",
       " 'contents',\n",
       " 'of',\n",
       " 'documents',\n",
       " ',',\n",
       " 'including',\n",
       " 'the',\n",
       " 'contextual',\n",
       " 'nuances',\n",
       " 'of',\n",
       " 'the',\n",
       " 'language',\n",
       " 'within',\n",
       " 'them',\n",
       " '.',\n",
       " 'The',\n",
       " 'technology',\n",
       " 'can',\n",
       " 'then',\n",
       " 'accurately',\n",
       " 'extract',\n",
       " 'information',\n",
       " 'and',\n",
       " 'insights',\n",
       " 'contained',\n",
       " 'in',\n",
       " 'the',\n",
       " 'documents',\n",
       " 'as',\n",
       " 'well',\n",
       " 'as',\n",
       " 'categorize',\n",
       " 'and',\n",
       " 'organize',\n",
       " 'the',\n",
       " 'documents',\n",
       " 'themselves',\n",
       " '.',\n",
       " 'Challenges',\n",
       " 'in',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'frequently',\n",
       " 'involve',\n",
       " 'speech',\n",
       " 'recognition',\n",
       " ',',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'understanding',\n",
       " ',',\n",
       " 'and',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'generation',\n",
       " '.',\n",
       " 'you',\n",
       " 'would',\n",
       " \"'ve\",\n",
       " 'had',\n",
       " 'done',\n",
       " 'that',\n",
       " ',',\n",
       " 'is',\n",
       " \"n't\",\n",
       " 'it',\n",
       " '?']"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "f270f86f-3118-4222-9755-d9c51451fbcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation as punc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "4868ebeb-9150-4abc-aea4-e6ea31047853",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "punc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "cfa5eb7a-4de8-4e11-a694-bbe5a19e0314",
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = [w for w in ws if w.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "83af0b1b-6ef4-4804-a529-2483ab5ff67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords as sw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "8dc1b3d4-1aff-4f65-ac63-511ecfee89d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "swe = sw.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "d7d9fd63-4c35-4c7a-bf24-a57ba1106485",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "swe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "5fcd8490-3b0f-4744-b799-daac83b40d76",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-96-ab1b6799843d>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-96-ab1b6799843d>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    remove punctuation\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "3a540a9b-90d0-448c-99fa-51073d594a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = [w for w in ws if w.isalpha() and w not in swe]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "6e46db48-f1ce-46ef-b4f3-71280b3e7dd9",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'article',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'done',\n",
       " 'computers',\n",
       " 'For',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'done',\n",
       " 'human',\n",
       " 'brain',\n",
       " 'see',\n",
       " 'Language',\n",
       " 'processing',\n",
       " 'brain',\n",
       " 'An',\n",
       " 'automated',\n",
       " 'online',\n",
       " 'assistant',\n",
       " 'providing',\n",
       " 'customer',\n",
       " 'service',\n",
       " 'web',\n",
       " 'page',\n",
       " 'example',\n",
       " 'application',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'major',\n",
       " 'component',\n",
       " 'Natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'NLP',\n",
       " 'subfield',\n",
       " 'linguistics',\n",
       " 'computer',\n",
       " 'science',\n",
       " 'artificial',\n",
       " 'intelligence',\n",
       " 'concerned',\n",
       " 'interactions',\n",
       " 'computers',\n",
       " 'human',\n",
       " 'language',\n",
       " 'particular',\n",
       " 'program',\n",
       " 'computers',\n",
       " 'process',\n",
       " 'analyze',\n",
       " 'large',\n",
       " 'amounts',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'data',\n",
       " 'The',\n",
       " 'goal',\n",
       " 'computer',\n",
       " 'capable',\n",
       " 'understanding',\n",
       " 'contents',\n",
       " 'documents',\n",
       " 'including',\n",
       " 'contextual',\n",
       " 'nuances',\n",
       " 'language',\n",
       " 'within',\n",
       " 'The',\n",
       " 'technology',\n",
       " 'accurately',\n",
       " 'extract',\n",
       " 'information',\n",
       " 'insights',\n",
       " 'contained',\n",
       " 'documents',\n",
       " 'well',\n",
       " 'categorize',\n",
       " 'organize',\n",
       " 'documents',\n",
       " 'Challenges',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'frequently',\n",
       " 'involve',\n",
       " 'speech',\n",
       " 'recognition',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'understanding',\n",
       " 'natural',\n",
       " 'language',\n",
       " 'generation',\n",
       " 'would',\n",
       " 'done']"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "f79ef3f5-1cb4-459c-9e8e-c3cb18ab3fdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\BOSS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "316c0541-915d-4751-9813-b5c0673691a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "3fdbde3a-72b8-4917-b01a-64805151d187",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "79825b4e-669d-4947-8deb-65516f1671e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt2 = '''Given a stream of text, determine which items in the text map to proper names, such as people or places, and what the type of each such name is (e.g. person, location, organization). Although capitalization can aid in recognizing named entities in languages such as English, this information cannot aid in determining the type of named entity, and in any case, is often inaccurate or insufficient. For example, the first letter of a sentence is also capitalized, and named entities often span several words, only some of which are capitalized. Furthermore, many other languages in non-Western scripts (e.g. Chinese or Arabic) do not have any capitalization at all, and even languages with capitalization may not consistently use it to distinguish names. For example, German capitalizes all nouns, regardless of whether they are names, and French and Spanish do not capitalize names that serve as adjectives.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "616727c4-46a6-4342-99c6-406b06a5547c",
   "metadata": {},
   "outputs": [],
   "source": [
    "txt3 = '''Given a sentence or larger chunk of text, determine which words (\"mentions\") refer to the same objects (\"entities\"). Anaphora resolution is a specific example of this task, and is specifically concerned with matching up pronouns with the nouns or names to which they refer. The more general task of coreference resolution also includes identifying so-called \"bridging relationships\" involving referring expressions. For example, in a sentence such as \"He entered John's house through the front door\", \"the front door\" is a referring expression and the bridging relationship to be identified is the fact that the door being referred to is the front door of John's house (rather than of some other structure that might also be referred to).'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "c0b1ad5a-ab4a-493f-af24-63c03dd9101f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(t):\n",
    "    words = []\n",
    "    for sent in nltk.sent_tokenize(t.lower()):    \n",
    "        ws = nltk.word_tokenize(sent)\n",
    "       # ws = [w for w in ws if w.isalpha() and w not in swe]\n",
    "        ws = [w for w in ws if w not in punc]    \n",
    "        words+=(ws)\n",
    "    words = ' '.join(words)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "68447551-171b-48c7-8df9-24ef108855c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"this article is about natural language processing done by computers for the natural language processing done by the human brain see language processing in the brain an automated online assistant providing customer service on a web page an example of an application where natural language processing is a major component natural language processing nlp is a subfield of linguistics computer science and artificial intelligence concerned with the interactions between computers and human language in particular how to program computers to process and analyze large amounts of natural language data the goal is a computer capable of `` understanding '' the contents of documents including the contextual nuances of the language within them the technology can then accurately extract information and insights contained in the documents as well as categorize and organize the documents themselves challenges in natural language processing frequently involve speech recognition natural language understanding and natural language generation you would 've had done that is n't it\",\n",
       " 'given a stream of text determine which items in the text map to proper names such as people or places and what the type of each such name is e.g person location organization although capitalization can aid in recognizing named entities in languages such as english this information can not aid in determining the type of named entity and in any case is often inaccurate or insufficient for example the first letter of a sentence is also capitalized and named entities often span several words only some of which are capitalized furthermore many other languages in non-western scripts e.g chinese or arabic do not have any capitalization at all and even languages with capitalization may not consistently use it to distinguish names for example german capitalizes all nouns regardless of whether they are names and french and spanish do not capitalize names that serve as adjectives',\n",
       " \"given a sentence or larger chunk of text determine which words `` mentions '' refer to the same objects `` entities '' anaphora resolution is a specific example of this task and is specifically concerned with matching up pronouns with the nouns or names to which they refer the more general task of coreference resolution also includes identifying so-called `` bridging relationships '' involving referring expressions for example in a sentence such as `` he entered john 's house through the front door '' `` the front door '' is a referring expression and the bridging relationship to be identified is the fact that the door being referred to is the front door of john 's house rather than of some other structure that might also be referred to\"]"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_list = []\n",
    "word_list.append(preprocess(txt))\n",
    "word_list.append(preprocess(txt2))\n",
    "word_list.append(preprocess(txt3))\n",
    "word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "aadafb86-c4e1-49d8-bb6d-0b86c005d98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemma = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "596e5731-3651-4b57-90db-b8f90331e989",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\BOSS\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "ec135fcc-a275-4346-87e2-f56faee84628",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('its', 'PRP$'),\n",
       " ('better', 'NN'),\n",
       " ('you', 'PRP'),\n",
       " ('do', 'VBP'),\n",
       " ('not', 'RB'),\n",
       " ('go', 'VB'),\n",
       " ('there', 'RB')]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag = nltk.pos_tag(nltk.word_tokenize('its better you do not go there'))\n",
    "tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "5408fb50-bcc5-481c-8221-10150fb8249c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('its', 'PRP$'),\n",
       " ('better', 'NN'),\n",
       " ('you', 'PRP'),\n",
       " ('do', 'VBP'),\n",
       " ('not', 'RB'),\n",
       " ('go', 'VB'),\n",
       " ('there', 'RB')]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "73553c5e-3cb6-417d-bf05-121b8ba2fa7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'well'"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma.lemmatize('better',wordnet.ADV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "8c00608d-e84a-4068-8be1-41537473d761",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'r'"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet.ADV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "dd8f698e-775b-4f2a-9350-adfb851ecb8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'a'"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnet.ADJ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "97306f6b-87bd-4b59-9e34-95de49375118",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'good'"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma.lemmatize('better',wordnet.ADJ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "8e790d20-c32c-4e13-8e2b-e55a29688741",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'better'"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemma.lemmatize('better',wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "2ccc182b-a53d-47f9-b7b9-2f1be48ee49a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'go'"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('going')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "34dd3f5d-2fc1-4bea-bae7-3d3ebeff9d89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'doe'"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('does')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "678637aa-8249-4e74-810e-e5d73dff0ff9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'work'"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer.stem('worked')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eae36c1-f5e7-4719-8354-db4fe2e969b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78d9702c-d630-4430-9cfc-69107adaa20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "a11fccb2-1eaa-4c0d-8410-b6934f56479f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "5f3bbb82-9f0d-468d-9727-13bf88b15b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow = cv.fit_transform(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "217f1350-ce45-4cf0-b238-aeb716818f97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x207 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 250 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1d060551-82d4-42ef-b1b9-93d3fcf4f1f6",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['about',\n",
       " 'accurately',\n",
       " 'adjectives',\n",
       " 'aid',\n",
       " 'all',\n",
       " 'also',\n",
       " 'although',\n",
       " 'amounts',\n",
       " 'an',\n",
       " 'analyze',\n",
       " 'anaphora',\n",
       " 'and',\n",
       " 'any',\n",
       " 'application',\n",
       " 'arabic',\n",
       " 'are',\n",
       " 'article',\n",
       " 'artificial',\n",
       " 'as',\n",
       " 'assistant',\n",
       " 'at',\n",
       " 'automated',\n",
       " 'be',\n",
       " 'being',\n",
       " 'between',\n",
       " 'brain',\n",
       " 'bridging',\n",
       " 'by',\n",
       " 'called',\n",
       " 'can',\n",
       " 'capable',\n",
       " 'capitalization',\n",
       " 'capitalize',\n",
       " 'capitalized',\n",
       " 'capitalizes',\n",
       " 'case',\n",
       " 'categorize',\n",
       " 'challenges',\n",
       " 'chinese',\n",
       " 'chunk',\n",
       " 'component',\n",
       " 'computer',\n",
       " 'computers',\n",
       " 'concerned',\n",
       " 'consistently',\n",
       " 'contained',\n",
       " 'contents',\n",
       " 'contextual',\n",
       " 'coreference',\n",
       " 'customer',\n",
       " 'data',\n",
       " 'determine',\n",
       " 'determining',\n",
       " 'distinguish',\n",
       " 'do',\n",
       " 'documents',\n",
       " 'done',\n",
       " 'door',\n",
       " 'each',\n",
       " 'english',\n",
       " 'entered',\n",
       " 'entities',\n",
       " 'entity',\n",
       " 'even',\n",
       " 'example',\n",
       " 'expression',\n",
       " 'expressions',\n",
       " 'extract',\n",
       " 'fact',\n",
       " 'first',\n",
       " 'for',\n",
       " 'french',\n",
       " 'frequently',\n",
       " 'front',\n",
       " 'furthermore',\n",
       " 'general',\n",
       " 'generation',\n",
       " 'german',\n",
       " 'given',\n",
       " 'goal',\n",
       " 'had',\n",
       " 'have',\n",
       " 'he',\n",
       " 'house',\n",
       " 'how',\n",
       " 'human',\n",
       " 'identified',\n",
       " 'identifying',\n",
       " 'in',\n",
       " 'inaccurate',\n",
       " 'includes',\n",
       " 'including',\n",
       " 'information',\n",
       " 'insights',\n",
       " 'insufficient',\n",
       " 'intelligence',\n",
       " 'interactions',\n",
       " 'involve',\n",
       " 'involving',\n",
       " 'is',\n",
       " 'it',\n",
       " 'items',\n",
       " 'john',\n",
       " 'language',\n",
       " 'languages',\n",
       " 'large',\n",
       " 'larger',\n",
       " 'letter',\n",
       " 'linguistics',\n",
       " 'location',\n",
       " 'major',\n",
       " 'many',\n",
       " 'map',\n",
       " 'matching',\n",
       " 'may',\n",
       " 'mentions',\n",
       " 'might',\n",
       " 'more',\n",
       " 'name',\n",
       " 'named',\n",
       " 'names',\n",
       " 'natural',\n",
       " 'nlp',\n",
       " 'non',\n",
       " 'not',\n",
       " 'nouns',\n",
       " 'nuances',\n",
       " 'objects',\n",
       " 'of',\n",
       " 'often',\n",
       " 'on',\n",
       " 'online',\n",
       " 'only',\n",
       " 'or',\n",
       " 'organization',\n",
       " 'organize',\n",
       " 'other',\n",
       " 'page',\n",
       " 'particular',\n",
       " 'people',\n",
       " 'person',\n",
       " 'places',\n",
       " 'process',\n",
       " 'processing',\n",
       " 'program',\n",
       " 'pronouns',\n",
       " 'proper',\n",
       " 'providing',\n",
       " 'rather',\n",
       " 'recognition',\n",
       " 'recognizing',\n",
       " 'refer',\n",
       " 'referred',\n",
       " 'referring',\n",
       " 'regardless',\n",
       " 'relationship',\n",
       " 'relationships',\n",
       " 'resolution',\n",
       " 'same',\n",
       " 'science',\n",
       " 'scripts',\n",
       " 'see',\n",
       " 'sentence',\n",
       " 'serve',\n",
       " 'service',\n",
       " 'several',\n",
       " 'so',\n",
       " 'some',\n",
       " 'span',\n",
       " 'spanish',\n",
       " 'specific',\n",
       " 'specifically',\n",
       " 'speech',\n",
       " 'stream',\n",
       " 'structure',\n",
       " 'subfield',\n",
       " 'such',\n",
       " 'task',\n",
       " 'technology',\n",
       " 'text',\n",
       " 'than',\n",
       " 'that',\n",
       " 'the',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'they',\n",
       " 'this',\n",
       " 'through',\n",
       " 'to',\n",
       " 'type',\n",
       " 'understanding',\n",
       " 'up',\n",
       " 'use',\n",
       " 've',\n",
       " 'web',\n",
       " 'well',\n",
       " 'western',\n",
       " 'what',\n",
       " 'where',\n",
       " 'whether',\n",
       " 'which',\n",
       " 'with',\n",
       " 'within',\n",
       " 'words',\n",
       " 'would',\n",
       " 'you']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uw = cv.get_feature_names()\n",
    "uw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a1609de-d5b2-4464-b87d-9bf42353e671",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bow' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-5fd96d037c1c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mbow_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mbow_df\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'bow' is not defined"
     ]
    }
   ],
   "source": [
    "bow_df = pd.DataFrame(bow.toarray(), columns=uw)\n",
    "bow_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3f9bea1c-eddf-4482-a70e-7b87fcd46d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ab845e2a-6b40-4685-815b-69be73e462fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfV = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c2583c14-3e56-4c23-927f-b212f62c24d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = tfidfV.fit_transform(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "dcecd6b5-0c43-4dc2-9161-00270e243d26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3x207 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 250 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "04209ba4-bc7a-4cae-b6c2-d2b7223caf18",
   "metadata": {},
   "outputs": [],
   "source": [
    "uw1 = tfidfV.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "5052a3a8-c045-43f3-aa22-be7f5e99846c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>about</th>\n",
       "      <th>accurately</th>\n",
       "      <th>adjectives</th>\n",
       "      <th>aid</th>\n",
       "      <th>all</th>\n",
       "      <th>also</th>\n",
       "      <th>although</th>\n",
       "      <th>amounts</th>\n",
       "      <th>an</th>\n",
       "      <th>analyze</th>\n",
       "      <th>...</th>\n",
       "      <th>western</th>\n",
       "      <th>what</th>\n",
       "      <th>where</th>\n",
       "      <th>whether</th>\n",
       "      <th>which</th>\n",
       "      <th>with</th>\n",
       "      <th>within</th>\n",
       "      <th>words</th>\n",
       "      <th>would</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.04848</td>\n",
       "      <td>0.04848</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.04848</td>\n",
       "      <td>0.14544</td>\n",
       "      <td>0.04848</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.04848</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.028633</td>\n",
       "      <td>0.04848</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.04848</td>\n",
       "      <td>0.04848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.068511</td>\n",
       "      <td>0.137022</td>\n",
       "      <td>0.137022</td>\n",
       "      <td>0.052104</td>\n",
       "      <td>0.068511</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.068511</td>\n",
       "      <td>0.068511</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.068511</td>\n",
       "      <td>0.104209</td>\n",
       "      <td>0.040464</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.052104</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.116118</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.116118</td>\n",
       "      <td>0.090176</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.058059</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows Ã— 207 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     about  accurately  adjectives       aid       all      also  although  \\\n",
       "0  0.04848     0.04848    0.000000  0.000000  0.000000  0.000000  0.000000   \n",
       "1  0.00000     0.00000    0.068511  0.137022  0.137022  0.052104  0.068511   \n",
       "2  0.00000     0.00000    0.000000  0.000000  0.000000  0.116118  0.000000   \n",
       "\n",
       "   amounts       an  analyze  ...   western      what    where   whether  \\\n",
       "0  0.04848  0.14544  0.04848  ...  0.000000  0.000000  0.04848  0.000000   \n",
       "1  0.00000  0.00000  0.00000  ...  0.068511  0.068511  0.00000  0.068511   \n",
       "2  0.00000  0.00000  0.00000  ...  0.000000  0.000000  0.00000  0.000000   \n",
       "\n",
       "      which      with   within     words    would      you  \n",
       "0  0.000000  0.028633  0.04848  0.000000  0.04848  0.04848  \n",
       "1  0.104209  0.040464  0.00000  0.052104  0.00000  0.00000  \n",
       "2  0.116118  0.090176  0.00000  0.058059  0.00000  0.00000  \n",
       "\n",
       "[3 rows x 207 columns]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_df = pd.DataFrame(tf_idf.toarray(), columns=uw1)\n",
    "tfidf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e477e5f7-d8bc-4e40-911c-d0737f5f8988",
   "metadata": {},
   "outputs": [],
   "source": [
    "sms spam prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ee11e41-3f1c-4267-9405-013bf5f77f91",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>msg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <td>body_text</td>\n",
       "      <td>body_text_nostop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ham</th>\n",
       "      <td>I've been searching for the right words to tha...</td>\n",
       "      <td>['ive', 'searching', 'right', 'words', 'thank'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spam</th>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>['free', 'entry', '2', 'wkly', 'comp', 'win', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ham</th>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>['nah', 'dont', 'think', 'goes', 'usf', 'lives...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ham</th>\n",
       "      <td>Even my brother is not like to speak with me. ...</td>\n",
       "      <td>['even', 'brother', 'like', 'speak', 'treat', ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    type  \\\n",
       "label                                          body_text   \n",
       "ham    I've been searching for the right words to tha...   \n",
       "spam   Free entry in 2 a wkly comp to win FA Cup fina...   \n",
       "ham    Nah I don't think he goes to usf, he lives aro...   \n",
       "ham    Even my brother is not like to speak with me. ...   \n",
       "\n",
       "                                                     msg  \n",
       "label                                   body_text_nostop  \n",
       "ham    ['ive', 'searching', 'right', 'words', 'thank'...  \n",
       "spam   ['free', 'entry', '2', 'wkly', 'comp', 'win', ...  \n",
       "ham    ['nah', 'dont', 'think', 'goes', 'usf', 'lives...  \n",
       "ham    ['even', 'brother', 'like', 'speak', 'treat', ...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('SMSSpamCollection_cleaned.tsv',sep='\\t',names=['type','msg'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "949964ae-760f-448b-89a7-dc4c0d241859",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>msg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <td>body_text_nostop</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ham</th>\n",
       "      <td>['ive', 'searching', 'right', 'words', 'thank'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spam</th>\n",
       "      <td>['free', 'entry', '2', 'wkly', 'comp', 'win', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ham</th>\n",
       "      <td>['nah', 'dont', 'think', 'goes', 'usf', 'lives...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ham</th>\n",
       "      <td>['even', 'brother', 'like', 'speak', 'treat', ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     msg\n",
       "label                                   body_text_nostop\n",
       "ham    ['ive', 'searching', 'right', 'words', 'thank'...\n",
       "spam   ['free', 'entry', '2', 'wkly', 'comp', 'win', ...\n",
       "ham    ['nah', 'dont', 'think', 'goes', 'usf', 'lives...\n",
       "ham    ['even', 'brother', 'like', 'speak', 'treat', ..."
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "1d788392-07ae-4fdd-9827-7b887094b9ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">msg</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>type</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>&amp;lt;#&amp;gt;  in mca. But not conform.</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>['', 'ltgt', 'mca', 'conform']</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&amp;lt;#&amp;gt;  mins but i had to stop somewhere first.</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>['', 'ltgt', 'mins', 'stop', 'somewhere', 'fir...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&amp;lt;DECIMAL&amp;gt; m but its not a common car here so its better to buy from china or asia. Or if i find it less expensive. I.ll holla</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>['', 'ltdecimalgt', 'common', 'car', 'better',...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and  picking them up from various points</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>['', 'picking', 'various', 'points']</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>came to look at the flat, seems ok, in his 50s? * Is away alot wiv work. Got woman coming at 6.30 too.</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>['', 'came', 'look', 'flat', 'seems', 'ok', '5...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ãœ thk of wat to eat tonight.</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>['Ã¼', 'thk', 'wat', 'eat', 'tonight']</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ãœ v ma fan...</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>['Ã¼', 'v', 'fan']</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ãœ wait 4 me in sch i finish ard 5..</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>['Ã¼', 'wait', '4', 'sch', 'finish', 'ard', '5']</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>â€¦ and donâ€˜t worry weâ€˜ll have finished by march â€¦ ish!</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>['', 'worry', 'finished', 'march', 'ish']</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>â€¦ we r stayin here an extra week, back next wed. How did we do in the rugby this weekend? Hi to and and , c u soon \"</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>['', 'r', 'stayin', 'extra', 'week', 'back', '...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5166 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     msg         \\\n",
       "                                                   count unique   \n",
       "type                                                              \n",
       " &lt;#&gt;  in mca. But not conform.                   1      1   \n",
       " &lt;#&gt;  mins but i had to stop somewhere fi...     1      1   \n",
       " &lt;DECIMAL&gt; m but its not a common car her...     1      1   \n",
       " and  picking them up from various points              1      1   \n",
       " came to look at the flat, seems ok, in his 50s...     1      1   \n",
       "...                                                  ...    ...   \n",
       "Ãœ thk of wat to eat tonight.                           1      1   \n",
       "Ãœ v ma fan...                                          1      1   \n",
       "Ãœ wait 4 me in sch i finish ard 5..                    1      1   \n",
       "â€¦ and donâ€˜t worry weâ€˜ll have finished by march ...     1      1   \n",
       "â€¦ we r stayin here an extra week, back next wed...     1      1   \n",
       "\n",
       "                                                                                                       \\\n",
       "                                                                                                  top   \n",
       "type                                                                                                    \n",
       " &lt;#&gt;  in mca. But not conform.                                   ['', 'ltgt', 'mca', 'conform']   \n",
       " &lt;#&gt;  mins but i had to stop somewhere fi...  ['', 'ltgt', 'mins', 'stop', 'somewhere', 'fir...   \n",
       " &lt;DECIMAL&gt; m but its not a common car her...  ['', 'ltdecimalgt', 'common', 'car', 'better',...   \n",
       " and  picking them up from various points                        ['', 'picking', 'various', 'points']   \n",
       " came to look at the flat, seems ok, in his 50s...  ['', 'came', 'look', 'flat', 'seems', 'ok', '5...   \n",
       "...                                                                                               ...   \n",
       "Ãœ thk of wat to eat tonight.                                    ['Ã¼', 'thk', 'wat', 'eat', 'tonight']   \n",
       "Ãœ v ma fan...                                                                       ['Ã¼', 'v', 'fan']   \n",
       "Ãœ wait 4 me in sch i finish ard 5..                   ['Ã¼', 'wait', '4', 'sch', 'finish', 'ard', '5']   \n",
       "â€¦ and donâ€˜t worry weâ€˜ll have finished by march ...          ['', 'worry', 'finished', 'march', 'ish']   \n",
       "â€¦ we r stayin here an extra week, back next wed...  ['', 'r', 'stayin', 'extra', 'week', 'back', '...   \n",
       "\n",
       "                                                         \n",
       "                                                   freq  \n",
       "type                                                     \n",
       " &lt;#&gt;  in mca. But not conform.                  1  \n",
       " &lt;#&gt;  mins but i had to stop somewhere fi...    1  \n",
       " &lt;DECIMAL&gt; m but its not a common car her...    1  \n",
       " and  picking them up from various points             1  \n",
       " came to look at the flat, seems ok, in his 50s...    1  \n",
       "...                                                 ...  \n",
       "Ãœ thk of wat to eat tonight.                          1  \n",
       "Ãœ v ma fan...                                         1  \n",
       "Ãœ wait 4 me in sch i finish ard 5..                   1  \n",
       "â€¦ and donâ€˜t worry weâ€˜ll have finished by march ...    1  \n",
       "â€¦ we r stayin here an extra week, back next wed...    1  \n",
       "\n",
       "[5166 rows x 4 columns]"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.groupby('type').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "3cbba577-9c07-4462-8914-269fd8205357",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 5569 entries, label to ham\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   type    5569 non-null   object\n",
      " 1   msg     5569 non-null   object\n",
      "dtypes: object(2)\n",
      "memory usage: 130.5+ KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "098d3544-3f79-42b2-bc3a-496784a7fe20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "body_text_nostop\n"
     ]
    }
   ],
   "source": [
    "for ind, m in data.iterrows():\n",
    "    print(m['msg'])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "211043e9-ec3e-47e2-be86-55a04ea1e9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = []\n",
    "for _, m in data.iterrows():\n",
    "    word_list.append(preprocess(m['msg']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "ca65ca38-58c3-4712-8fe7-17e4d703808a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ham'"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "c8bae7ff-88ae-46de-b033-bbbfbda19379",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidfV = TfidfVectorizer()\n",
    "tf_idf = tfidfV.fit_transform(word_list)\n",
    "uw1 = tfidfV.get_feature_names()\n",
    "tfidf_df = pd.DataFrame(tf_idf.toarray(), columns=uw1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "ad4f8bb9-b1cc-4c7e-876d-d96d3bd0ce66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>008704050406</th>\n",
       "      <th>0089my</th>\n",
       "      <th>0121</th>\n",
       "      <th>01223585236</th>\n",
       "      <th>01223585334</th>\n",
       "      <th>0125698789</th>\n",
       "      <th>02</th>\n",
       "      <th>020603</th>\n",
       "      <th>0207</th>\n",
       "      <th>02070836089</th>\n",
       "      <th>...</th>\n",
       "      <th>zeros</th>\n",
       "      <th>zhong</th>\n",
       "      <th>zindgi</th>\n",
       "      <th>zoe</th>\n",
       "      <th>zogtorius</th>\n",
       "      <th>zoom</th>\n",
       "      <th>zouk</th>\n",
       "      <th>zyada</th>\n",
       "      <th>Ã¼ll</th>\n",
       "      <th>ã€¨ud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5564</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5565</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5566</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5569 rows Ã— 9412 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      008704050406  0089my  0121  01223585236  01223585334  0125698789   02  \\\n",
       "0              0.0     0.0   0.0          0.0          0.0         0.0  0.0   \n",
       "1              0.0     0.0   0.0          0.0          0.0         0.0  0.0   \n",
       "2              0.0     0.0   0.0          0.0          0.0         0.0  0.0   \n",
       "3              0.0     0.0   0.0          0.0          0.0         0.0  0.0   \n",
       "4              0.0     0.0   0.0          0.0          0.0         0.0  0.0   \n",
       "...            ...     ...   ...          ...          ...         ...  ...   \n",
       "5564           0.0     0.0   0.0          0.0          0.0         0.0  0.0   \n",
       "5565           0.0     0.0   0.0          0.0          0.0         0.0  0.0   \n",
       "5566           0.0     0.0   0.0          0.0          0.0         0.0  0.0   \n",
       "5567           0.0     0.0   0.0          0.0          0.0         0.0  0.0   \n",
       "5568           0.0     0.0   0.0          0.0          0.0         0.0  0.0   \n",
       "\n",
       "      020603  0207  02070836089  ...  zeros  zhong  zindgi  zoe  zogtorius  \\\n",
       "0        0.0   0.0          0.0  ...    0.0    0.0     0.0  0.0        0.0   \n",
       "1        0.0   0.0          0.0  ...    0.0    0.0     0.0  0.0        0.0   \n",
       "2        0.0   0.0          0.0  ...    0.0    0.0     0.0  0.0        0.0   \n",
       "3        0.0   0.0          0.0  ...    0.0    0.0     0.0  0.0        0.0   \n",
       "4        0.0   0.0          0.0  ...    0.0    0.0     0.0  0.0        0.0   \n",
       "...      ...   ...          ...  ...    ...    ...     ...  ...        ...   \n",
       "5564     0.0   0.0          0.0  ...    0.0    0.0     0.0  0.0        0.0   \n",
       "5565     0.0   0.0          0.0  ...    0.0    0.0     0.0  0.0        0.0   \n",
       "5566     0.0   0.0          0.0  ...    0.0    0.0     0.0  0.0        0.0   \n",
       "5567     0.0   0.0          0.0  ...    0.0    0.0     0.0  0.0        0.0   \n",
       "5568     0.0   0.0          0.0  ...    0.0    0.0     0.0  0.0        0.0   \n",
       "\n",
       "      zoom  zouk  zyada  Ã¼ll  ã€¨ud  \n",
       "0      0.0   0.0    0.0  0.0  0.0  \n",
       "1      0.0   0.0    0.0  0.0  0.0  \n",
       "2      0.0   0.0    0.0  0.0  0.0  \n",
       "3      0.0   0.0    0.0  0.0  0.0  \n",
       "4      0.0   0.0    0.0  0.0  0.0  \n",
       "...    ...   ...    ...  ...  ...  \n",
       "5564   0.0   0.0    0.0  0.0  0.0  \n",
       "5565   0.0   0.0    0.0  0.0  0.0  \n",
       "5566   0.0   0.0    0.0  0.0  0.0  \n",
       "5567   0.0   0.0    0.0  0.0  0.0  \n",
       "5568   0.0   0.0    0.0  0.0  0.0  \n",
       "\n",
       "[5569 rows x 9412 columns]"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "15c49e1a-792c-453c-9a31-bf24487d19b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "c74b759e-dd1e-433f-861f-140d73c8865b",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, xtest, ytrain, ytest = train_test_split(tfidf_df, data['type'], test_size=0.25, random_state=85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "f86778d5-7845-46d3-a24b-8df51f7f5268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BernoulliNB: 0.005025125628140704\n",
      "\n",
      "MultinomialNB: 0.00933237616654702\n",
      "\n",
      "GaussianNB: 0.10839913854989232\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model_b = BernoulliNB().fit(xtrain,ytrain)\n",
    "print('BernoulliNB:',model_b.score(xtest,ytest))\n",
    "print('')\n",
    "\n",
    "model_m = MultinomialNB().fit(xtrain,ytrain)\n",
    "print('MultinomialNB:',model_m.score(xtest,ytest))\n",
    "print('')\n",
    "\n",
    "model_g = GaussianNB().fit(xtrain,ytrain)\n",
    "print('GaussianNB:',model_g.score(xtest,ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc0da6d-22a5-4917-a006-9ab57ec9bebe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
